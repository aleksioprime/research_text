{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85b4696d-787f-4e3e-8558-f01b6acfbba8",
   "metadata": {},
   "source": [
    "# Метрика TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5652bf-a243-43f0-b188-a2d03e550ef9",
   "metadata": {},
   "source": [
    "**TF-IDF (Term Frequency-Inverse Document Frequency)** — это метрика, используемая в текстовой аналитике для оценки важности слова в документе относительно всего корпуса документов.\n",
    "\n",
    "- **TF (Term Frequency)**: Частота слова в документе. Чем чаще слово встречается в документе, тем выше его значение TF. Если слово \"машина\" встречается в документе 5 раз, а в документе всего 100 слов, то TF(\"машина\") = 5/100 = 0.05.\n",
    "- **IDF (Inverse Document Frequency)**: Обратная частота документа. Эта часть метрики указывает на редкость слова в корпусе документов. Чем реже слово встречается в других документах, тем выше его значение IDF. Если слово \"машина\" встречается в 3 из 100 документов, то IDF(\"машина\") = log(100/3).\n",
    "\n",
    "TF-IDF рассчитывается как произведение TF и IDF:\n",
    "\n",
    "TF-IDF(t,d)=TF(t,d)×IDF(t), где t — слово, d — документ, а N — общее число документов.\n",
    "\n",
    "Предположим, у нас есть 4 документа и мы хотим оценить важность слова \"машина\":\n",
    "\n",
    "TF:\n",
    "- В документе 1 слово \"машина\" встречается 5 раз из 100 слов, TF = 0.05.\n",
    "- В документе 2 слово \"машина\" встречается 3 раза из 50 слов, TF = 0.06.\n",
    "  \n",
    "IDF:\n",
    "- Если \"машина\" встречается в 3 из 4 документов, IDF = log(4/3).\n",
    "  \n",
    "TF-IDF:\n",
    "- Для документа 1: TF-IDF = 0.05 * log(4/3).\n",
    "- Для документа 2: TF-IDF = 0.06 * log(4/3).\n",
    "  \n",
    "TF-IDF используется в задачах:\n",
    "- Поиск информации: TF-IDF помогает найти наиболее релевантные документы по запросу пользователя. Чем выше TF-IDF слова в документе, тем более важным считается это слово для данного документа.\n",
    "- Тематическое моделирование: Определение тем в тексте и кластеризация документов по темам.\n",
    "- Анализ текста: Помогает в выявлении ключевых слов и фраз в больших объемах текста."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d068e8a-13f7-4fe0-905d-0574abbc7f73",
   "metadata": {},
   "source": [
    "## Рассчёт TF-IDF корпуса документов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2488e284-fed5-4b33-9947-5f29ffb715af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pymorphy2\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caffffe0-8a57-4a6d-90cd-8113782bcb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"В последние десятилетия интеллектуальные технологии стали неотъемлемой частью многих сфер жизни, включая образование, здравоохранение, финансы и промышленность. Машинное обучение и искусственный интеллект продолжают развиваться, внедряясь в различные аспекты нашего общества и экономики. Рост объема данных, доступных для анализа, создает большие возможности для применения технологий машинного обучения и искусственного интеллекта в повседневной жизни.\",\n",
    "    \"Современные системы искусственного интеллекта, такие как нейронные сети и глубокое обучение, обладают потенциалом для автоматизации процессов принятия решений, улучшения производительности и предоставления персонализированных услуг. Однако с ростом использования этих технологий возникают новые вызовы, такие как проблемы конфиденциальности данных, этические вопросы и неопределенность в прогнозах.\",\n",
    "    \"Подходы к решению этих проблем включают в себя разработку новых методов обработки и анализа данных, улучшение алгоритмов машинного обучения и создание строгих правил для защиты конфиденциальности и безопасности данных. Кроме того, важно проводить обучение и образование в области искусственного интеллекта, чтобы повысить осведомленность и компетентность в этой области.\",\n",
    "    \"Одним из ключевых направлений развития искусственного интеллекта является создание автономных систем, способных принимать решения и действовать без участия человека. Эти системы могут использоваться в различных областях, таких как автопилоты для автомобилей, управление трафиком, медицинские диагнозы и рекомендательные системы.\",\n",
    "    \"Однако внедрение автономных систем также вызывает вопросы безопасности и этики, связанные с возможностью ошибок и неспособностью системы объяснить свои решения. Поэтому важно разрабатывать методы верификации и валидации автономных систем, а также устанавливать четкие правила использования и ответственности за их действия.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb80791f-f2a8-4f26-aaf5-5362fe80dc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"Машина едет по дороге.\",\n",
    "    \"Водитель управляет машиной.\",\n",
    "    \"Дорога прямая и свободная.\",\n",
    "    \"Машина остановилась на светофоре.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a947a4a7-5150-4c8b-9f03-3c70cdff7c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка стоп-слов и установка токенизатора\n",
    "stop_words = set(stopwords.words('russian'))\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "# Функция для лемматизации текста\n",
    "def lemmatize(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    lemmas = [morph.parse(token)[0].normal_form for token in tokens if token.isalnum() and token not in stop_words]\n",
    "    return ' '.join(lemmas)\n",
    "\n",
    "processed_documents = [lemmatize(doc) for doc in documents]\n",
    "\n",
    "# Применение TF-IDF для трансформации корпуса\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(processed_documents)\n",
    "\n",
    "# Получение списка слов\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Преобразование TF-IDF матрицы в DataFrame для удобства отображения\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "# Вывод результатов\n",
    "display(tfidf_df.style.format(precision=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88917870-2ae6-4f6d-8e75-60cdd7da862c",
   "metadata": {},
   "source": [
    "### Определение ключевых слов для каждого документа "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5be9f5-4dc4-43d0-bfad-59b247ed18c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для определения ключевых слов для каждого документа\n",
    "def get_top_keywords(tfidf_matrix, feature_names, top_n=5):\n",
    "    top_keywords = []\n",
    "    for row in tfidf_matrix:\n",
    "        row_array = row.toarray().flatten()\n",
    "        top_indices = row_array.argsort()[-top_n:][::-1]\n",
    "        top_words = [feature_names[idx] for idx in top_indices]\n",
    "        top_keywords.append(top_words)\n",
    "    return top_keywords\n",
    "    \n",
    "# Определение ключевых слов\n",
    "top_keywords = get_top_keywords(tfidf_matrix, feature_names)\n",
    "\n",
    "# Вывод ключевых слов для каждого документа\n",
    "for doc_index, keywords in enumerate(top_keywords):\n",
    "    print(f\"{doc_index + 1}. Ключевые слова: {', '.join(keywords)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f14ecef-8fa9-483d-8499-1e888eb5fb5a",
   "metadata": {},
   "source": [
    "### Определение сходства документов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9562e579-6ef7-47e3-b2e7-2027dfc61ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarities = cosine_similarity(tfidf_matrix)\n",
    "cosine_sim_df = pd.DataFrame(cosine_similarities, index=[f\"Документ {i+1}\" for i in range(len(documents))], columns=[f\"Документ {i+1}\" for i in range(len(documents))])\n",
    "print(\"Сходство документов:\\n\")\n",
    "print(cosine_sim_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41397818-37aa-4c0f-86fb-b64e5a0c0c2f",
   "metadata": {},
   "source": [
    "### Создание облака слов для каждого документа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dff14c-8480-49b0-a91a-5e58e9707421",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Функция для создания облака слов для каждого документа\n",
    "def create_wordcloud(text, title):\n",
    "    wordcloud = WordCloud(width=400, height=300, background_color='white').generate(text)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "\n",
    "# Визуализация облаков слов в виде сетки\n",
    "num_docs = len(documents)\n",
    "grid_size = int(np.ceil(np.sqrt(num_docs)))\n",
    "fig, axes = plt.subplots(grid_size, grid_size, figsize=(15, 10))\n",
    "\n",
    "for i, doc in enumerate(processed_documents):\n",
    "    row = i // grid_size\n",
    "    col = i % grid_size\n",
    "    ax = axes[row, col]\n",
    "    wordcloud = WordCloud(width=400, height=300, background_color='white').generate(doc)\n",
    "    ax.imshow(wordcloud, interpolation='bilinear')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f\"Документ {i+1}\")\n",
    "\n",
    "# Удаление пустых осей\n",
    "for i in range(num_docs, grid_size * grid_size):\n",
    "    fig.delaxes(axes.flatten()[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d533e6a-bbed-4597-b22a-6b49dee01a83",
   "metadata": {},
   "source": [
    "### Создание общего текста для облака слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0f019b-60e7-4e0f-8bfb-04c4229f9e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "combined_text = ' '.join(processed_documents)\n",
    "\n",
    "# Визуализация общего облака слов\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(combined_text)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title(\"Общее облако слов для всех документов\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9a401a-9f57-427c-b8de-d9a8fc38f394",
   "metadata": {},
   "source": [
    "## Темы исследовательских работ\n",
    "\n",
    "1. Анализ ключевых слов в текстах разных жанров\n",
    "\n",
    "Ученики могут собрать тексты различных жанров (например, новости, научные статьи, художественная литература) и использовать TF-IDF для определения ключевых слов в каждом жанре.\n",
    "\n",
    "Исследовательские вопросы:\n",
    "- Какие слова наиболее часто встречаются в новостных статьях по сравнению с научными статьями?\n",
    "- Как различаются ключевые слова в художественной литературе и научной литературе?\n",
    "\n",
    "2. Изменение ключевых слов с течением времени\n",
    "\n",
    "Ученики могут взять тексты, написанные в разные исторические периоды (например, газеты или книги) и проанализировать, как менялись ключевые слова с течением времени.\n",
    "\n",
    "Исследовательские вопросы:\n",
    "- Какие слова были популярны в разное время?\n",
    "- Как изменялись ключевые темы и понятия с течением времени?\n",
    "\n",
    "3. Сравнение ключевых слов в текстах разных авторов\n",
    "\n",
    "Ученики могут сравнить тексты разных авторов, чтобы понять уникальные стили и темы, которые они используют.\n",
    "\n",
    "Исследовательские вопросы:\n",
    "- Какие слова и темы уникальны для каждого автора?\n",
    "- Как различаются ключевые слова между авторами одного жанра?\n",
    "\n",
    "4. Анализ текста социальных сетей\n",
    "\n",
    "Ученики могут анализировать тексты из социальных сетей (например, посты в Twitter или Instagram) для выявления популярных тем и слов.\n",
    "\n",
    "Исследовательские вопросы:\n",
    "- Какие темы наиболее популярны в социальных сетях?\n",
    "- Как меняются популярные темы со временем?\n",
    "\n",
    "5. Выявление плагиата\n",
    "\n",
    "Ученики могут использовать TF-IDF для выявления плагиата, сравнивая тексты и определяя, насколько они похожи.\n",
    "\n",
    "Исследовательские вопросы:\n",
    "- Насколько схожи два текста по своим ключевым словам?\n",
    "- Можно ли определить плагиат, используя TF-IDF?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3fd8af-ecd7-4655-be73-a2eb85cc8474",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
